#!/bin/bash

# Flash-Attention å®‰è£…è„šæœ¬
# æ”¯æŒå¤šç§å®‰è£…æ–¹æ³•ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ–¹æ¡ˆ
# åˆ›å»ºæ—¶é—´: $(date)
# é€‚ç”¨ç¯å¢ƒ: XVerse Dockerå®¹å™¨

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "ğŸš€ Flash-Attention å®‰è£…è„šæœ¬å¯åŠ¨"
echo "================================="

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥ç¯å¢ƒå‡½æ•°
check_environment() {
    log_info "æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ..."
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    PYTHON_VERSION=$(python -c "import sys; print('.'.join(map(str, sys.version_info[:2])))")
    log_info "Pythonç‰ˆæœ¬: $PYTHON_VERSION"
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬
    TORCH_VERSION=$(python -c "import torch; print(torch.__version__)" 2>/dev/null || echo "æœªå®‰è£…")
    log_info "PyTorchç‰ˆæœ¬: $TORCH_VERSION"
    
    # æ£€æŸ¥CUDAç‰ˆæœ¬
    if command -v nvcc &> /dev/null; then
        CUDA_VERSION=$(nvcc --version | grep "release" | sed 's/.*release \([0-9.]*\).*/\1/')
        log_info "CUDAç‰ˆæœ¬: $CUDA_VERSION"
    else
        CUDA_VERSION="æœªæ‰¾åˆ°"
        log_warning "æœªæ‰¾åˆ°CUDAç¼–è¯‘å™¨"
    fi
    
    # æ£€æŸ¥torch CUDAç‰ˆæœ¬
    TORCH_CUDA=$(python -c "import torch; print(torch.version.cuda)" 2>/dev/null || echo "æœªçŸ¥")
    log_info "Torch CUDAç‰ˆæœ¬: $TORCH_CUDA"
    
    # æ£€æŸ¥GPUæ¶æ„
    if command -v nvidia-smi &> /dev/null; then
        GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -1)
        log_info "GPU: $GPU_NAME"
    else
        log_warning "æœªæ‰¾åˆ°nvidia-smi"
    fi
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    DISK_FREE=$(df -h / | awk 'NR==2 {print $4}')
    log_info "å¯ç”¨ç£ç›˜ç©ºé—´: $DISK_FREE"
    
    echo ""
}

# æ¸…ç†å‡½æ•°
cleanup_cache() {
    log_info "æ¸…ç†ç¼“å­˜ä»¥é‡Šæ”¾ç©ºé—´..."
    
    # æ¸…ç†pipç¼“å­˜
    pip cache purge 2>/dev/null || true
    
    # æ¸…ç†ç³»ç»Ÿç¼“å­˜
    if [ "$EUID" -eq 0 ]; then
        apt-get clean 2>/dev/null || true
        rm -rf /var/lib/apt/lists/* 2>/dev/null || true
    fi
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    rm -rf /tmp/* /var/tmp/* ~/.cache 2>/dev/null || true
    
    # æ¸…ç†Pythonç¼“å­˜
    find /usr/local -name "*.pyc" -delete 2>/dev/null || true
    find /usr/local -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
    
    log_success "ç¼“å­˜æ¸…ç†å®Œæˆ"
    df -h / | head -2
    echo ""
}

# æµ‹è¯•Flash-Attentionå®‰è£…
test_flash_attn() {
    log_info "æµ‹è¯•Flash-Attentionå®‰è£…..."
    
    python -c "
import sys
try:
    import flash_attn
    print('âœ… flash-attnå¯¼å…¥æˆåŠŸ')
    print(f'ç‰ˆæœ¬: {flash_attn.__version__}')
    
    from flash_attn import flash_attn_func
    print('âœ… flash_attn_funcå¯ç”¨')
    
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input
    print('âœ… bert_paddingæ¨¡å—å¯ç”¨')
    
    print('ğŸ‰ Flash-Attentionå®‰è£…æˆåŠŸå¹¶åŠŸèƒ½å®Œæ•´!')
    sys.exit(0)
    
except ImportError as e:
    print(f'âŒ Flash-Attentionå¯¼å…¥å¤±è´¥: {e}')
    sys.exit(1)
except Exception as e:
    print(f'âš ï¸ Flash-Attentionéƒ¨åˆ†åŠŸèƒ½å¼‚å¸¸: {e}')
    sys.exit(2)
"
    
    return $?
}

# æ–¹æ³•1: é¢„ç¼–è¯‘wheelå®‰è£…ï¼ˆæ¨èï¼‰
install_precompiled() {
    log_info "æ–¹æ³•1: ä½¿ç”¨é¢„ç¼–è¯‘wheelå®‰è£…Flash-Attention..."
    
    # æ–¹æ¡ˆ1: VLLMä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæœ€ç¨³å®šï¼‰
    log_info "å°è¯•å®‰è£…vllm-flash-attn..."
    if pip install --no-cache-dir vllm-flash-attn==2.6.2; then
        log_success "vllm-flash-attnå®‰è£…æˆåŠŸ"
        return 0
    fi
    
    # æ–¹æ¡ˆ2: å®˜æ–¹é¢„ç¼–è¯‘ç‰ˆæœ¬
    log_info "å°è¯•å®‰è£…å®˜æ–¹é¢„ç¼–è¯‘ç‰ˆæœ¬..."
    if pip install --no-cache-dir flash-attn==2.6.3 --find-links https://download.pytorch.org/whl/torch_stable.html; then
        log_success "å®˜æ–¹é¢„ç¼–è¯‘ç‰ˆæœ¬å®‰è£…æˆåŠŸ"
        return 0
    fi
    
    # æ–¹æ¡ˆ3: ä¸“ç”¨CUDA 11.8ç‰ˆæœ¬
    log_info "å°è¯•å®‰è£…CUDA 11.8ä¸“ç”¨ç‰ˆæœ¬..."
    WHEEL_URL="https://github.com/jllllll/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1%2Bcu118torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl"
    if pip install --no-cache-dir "$WHEEL_URL"; then
        log_success "CUDA 11.8ä¸“ç”¨ç‰ˆæœ¬å®‰è£…æˆåŠŸ"
        return 0
    fi
    
    # æ–¹æ¡ˆ4: æœ€æ–°ç¨³å®šç‰ˆæœ¬
    log_info "å°è¯•å®‰è£…æœ€æ–°ç¨³å®šç‰ˆæœ¬..."
    if pip install --no-cache-dir flash-attn==2.7.4.post1; then
        log_success "æœ€æ–°ç¨³å®šç‰ˆæœ¬å®‰è£…æˆåŠŸ"
        return 0
    fi
    
    log_error "æ‰€æœ‰é¢„ç¼–è¯‘ç‰ˆæœ¬å®‰è£…å¤±è´¥"
    return 1
}

# æ–¹æ³•2: æºç ç¼–è¯‘å®‰è£…
install_from_source() {
    log_info "æ–¹æ³•2: ä»æºç ç¼–è¯‘å®‰è£…Flash-Attention..."
    log_warning "æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦1-3å°æ—¶ï¼Œè¯·è€å¿ƒç­‰å¾…..."
    
    # è®¾ç½®ç¼–è¯‘ç¯å¢ƒå˜é‡
    export TORCH_CUDA_ARCH_LIST="7.0;7.5;8.0;8.6;8.9;9.0"
    export FLASH_ATTENTION_FORCE_BUILD=TRUE
    export MAX_JOBS=4  # é™åˆ¶å¹¶å‘æ•°é¿å…å†…å­˜ä¸è¶³
    
    log_info "ç¼–è¯‘ç¯å¢ƒå˜é‡å·²è®¾ç½®"
    log_info "TORCH_CUDA_ARCH_LIST=$TORCH_CUDA_ARCH_LIST"
    
    # ä»æºç å®‰è£…
    if pip install --no-cache-dir --no-build-isolation flash-attn==2.7.4.post1; then
        log_success "æºç ç¼–è¯‘å®‰è£…æˆåŠŸ"
        return 0
    else
        log_error "æºç ç¼–è¯‘å®‰è£…å¤±è´¥"
        return 1
    fi
}

# æ–¹æ³•3: Condaå®‰è£…
install_with_conda() {
    log_info "æ–¹æ³•3: ä½¿ç”¨Condaå®‰è£…Flash-Attention..."
    
    # æ£€æŸ¥condaæ˜¯å¦å¯ç”¨
    if ! command -v conda &> /dev/null; then
        log_warning "Condaæœªå®‰è£…ï¼Œè·³è¿‡æ­¤æ–¹æ³•"
        return 1
    fi
    
    # ä½¿ç”¨condaå®‰è£…
    if conda install -y flash-attn -c conda-forge; then
        log_success "Condaå®‰è£…æˆåŠŸ"
        return 0
    else
        log_error "Condaå®‰è£…å¤±è´¥"
        return 1
    fi
}

# ä¸»å®‰è£…æµç¨‹
main() {
    echo "ğŸ¯ å¼€å§‹Flash-Attentionå®‰è£…æµç¨‹"
    echo ""
    
    # æ£€æŸ¥ç¯å¢ƒ
    check_environment
    
    # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
    if test_flash_attn; then
        log_success "Flash-Attentionå·²å®‰è£…ä¸”åŠŸèƒ½æ­£å¸¸"
        echo ""
        echo "ğŸ‰ å®‰è£…æ£€æŸ¥å®Œæˆ!"
        exit 0
    fi
    
    # æ¸…ç†ç¼“å­˜
    cleanup_cache
    
    # å°è¯•æ–¹æ³•1: é¢„ç¼–è¯‘å®‰è£…
    log_info "å¼€å§‹å°è¯•é¢„ç¼–è¯‘å®‰è£…..."
    if install_precompiled; then
        if test_flash_attn; then
            log_success "ğŸ‰ é¢„ç¼–è¯‘å®‰è£…æˆåŠŸ!"
            exit 0
        else
            log_warning "é¢„ç¼–è¯‘å®‰è£…å®Œæˆä½†æµ‹è¯•å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•"
        fi
    fi
    
    # æ¸…ç†å¤±è´¥çš„å®‰è£…
    pip uninstall -y flash-attn vllm-flash-attn 2>/dev/null || true
    cleanup_cache
    
    # å°è¯•æ–¹æ³•2: æºç ç¼–è¯‘
    log_info "å¼€å§‹å°è¯•æºç ç¼–è¯‘..."
    if install_from_source; then
        if test_flash_attn; then
            log_success "ğŸ‰ æºç ç¼–è¯‘å®‰è£…æˆåŠŸ!"
            exit 0
        else
            log_warning "æºç ç¼–è¯‘å®Œæˆä½†æµ‹è¯•å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•"
        fi
    fi
    
    # æ¸…ç†å¤±è´¥çš„å®‰è£…
    pip uninstall -y flash-attn 2>/dev/null || true
    cleanup_cache
    
    # å°è¯•æ–¹æ³•3: Condaå®‰è£…
    log_info "å¼€å§‹å°è¯•Condaå®‰è£…..."
    if install_with_conda; then
        if test_flash_attn; then
            log_success "ğŸ‰ Condaå®‰è£…æˆåŠŸ!"
            exit 0
        else
            log_warning "Condaå®‰è£…å®Œæˆä½†æµ‹è¯•å¤±è´¥"
        fi
    fi
    
    # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
    log_error "âŒ æ‰€æœ‰å®‰è£…æ–¹æ³•éƒ½å¤±è´¥äº†"
    log_info "å»ºè®®:"
    log_info "1. æ£€æŸ¥CUDAç¯å¢ƒæ˜¯å¦æ­£ç¡®å®‰è£…"
    log_info "2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å’Œå†…å­˜"
    log_info "3. å°è¯•ä½¿ç”¨è¾ƒä½ç‰ˆæœ¬çš„flash-attn"
    log_info "4. é¡¹ç›®å¯ä»¥åœ¨æ²¡æœ‰flash-attnçš„æƒ…å†µä¸‹è¿è¡Œï¼ˆä½¿ç”¨PyTorchåŸç”Ÿattentionï¼‰"
    
    exit 1
}

# è„šæœ¬å…¥å£
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi 