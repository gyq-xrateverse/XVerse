# Flash-Attention å®‰è£…æŒ‡å—

## ğŸš€ å¿«é€Ÿå®‰è£… (æ¨è)

### Dockerå®¹å™¨å†…å®‰è£…

```bash
# 1. æ„å»ºå¹¶å¯åŠ¨å®¹å™¨
docker build -t xverse:optimized .
docker run -it --gpus all xverse:optimized bash

# 2. å¿«é€Ÿå®‰è£…Flash-Attention
bash scripts/quick_install_flash_attn.sh
```

### æœ¬åœ°ç¯å¢ƒå®‰è£…

```bash
# Linux/WSL/Windowsç¯å¢ƒ
bash scripts/quick_install_flash_attn.sh
```

## ğŸ“‹ å®‰è£…è„šæœ¬

**`quick_install_flash_attn.sh`** - ç®€å•é«˜æ•ˆçš„Flash-Attentionå®‰è£…è„šæœ¬

## ğŸ”§ æ‰‹åŠ¨å®‰è£…æ–¹æ³•

å¦‚æœè„šæœ¬å®‰è£…å¤±è´¥ï¼Œå¯ä»¥æ‰‹åŠ¨å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š

### æ–¹æ³•1: VLLMä¼˜åŒ–ç‰ˆ (æœ€æ¨è)
```bash
pip install --no-cache-dir vllm-flash-attn==2.6.2
```

### æ–¹æ³•2: å®˜æ–¹é¢„ç¼–è¯‘ç‰ˆ
```bash
pip install --no-cache-dir flash-attn==2.6.3 --find-links https://download.pytorch.org/whl/torch_stable.html
```

### æ–¹æ³•3: æœ€æ–°ç¨³å®šç‰ˆ
```bash
pip install --no-cache-dir flash-attn==2.7.4.post1
```

## âœ… éªŒè¯å®‰è£…

```python
# æµ‹è¯•å¯¼å…¥
import flash_attn
print(f"Flash-Attentionç‰ˆæœ¬: {flash_attn.__version__}")

# æµ‹è¯•åŠŸèƒ½
from flash_attn import flash_attn_func
print("âœ… Flash-AttentionåŠŸèƒ½æ­£å¸¸")
```

## â“ å¸¸è§é—®é¢˜

### Q: æ²¡æœ‰Flash-Attentioné¡¹ç›®èƒ½è¿è¡Œå—ï¼Ÿ
**A: å¯ä»¥æ­£å¸¸è¿è¡Œ**ã€‚é¡¹ç›®å·²å®ç°ä¼˜é›…é™çº§ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ°PyTorchåŸç”Ÿattentionã€‚

### Q: æ€§èƒ½å·®å¼‚æœ‰å¤šå¤§ï¼Ÿ
**A: Flash-Attentionæä¾›2-4å€é€Ÿåº¦æå‡å’Œ30-50%å†…å­˜ä¼˜åŒ–**ã€‚

### Q: å®‰è£…å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
**A: æŒ‰é¡ºåºå°è¯•**ï¼š
1. æ¸…ç†ç¼“å­˜ï¼š`pip cache purge`
2. æ£€æŸ¥CUDAç‰ˆæœ¬ï¼š`nvcc --version`
3. æ‰‹åŠ¨å°è¯•ä¸Šè¿°å®‰è£…æ–¹æ³•
4. é¡¹ç›®å¯ä»¥åœ¨æ²¡æœ‰Flash-Attentionçš„æƒ…å†µä¸‹æ­£å¸¸è¿è¡Œ

---
**é€‚ç”¨ç¯å¢ƒ**: XVerse Dockeré•œåƒ | **PyTorchç‰ˆæœ¬**: 2.6.0+cu118 