#!/usr/bin/env python3
"""
Flash-Attention Pythonå®‰è£…è„šæœ¬
æ”¯æŒå¤šç§å®‰è£…æ–¹æ³•ï¼Œæ™ºèƒ½ç¯å¢ƒæ£€æµ‹
"""

import subprocess
import sys
import os
import shutil
from pathlib import Path

def run_command(cmd, check=True, capture_output=False):
    """æ‰§è¡Œå‘½ä»¤å¹¶å¤„ç†ç»“æœ"""
    try:
        if capture_output:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            return result.returncode == 0, result.stdout.strip(), result.stderr.strip()
        else:
            result = subprocess.run(cmd, shell=True, check=check)
            return True, "", ""
    except subprocess.CalledProcessError as e:
        return False, "", str(e)

def log(message, level="INFO"):
    """æ—¥å¿—è¾“å‡º"""
    colors = {
        "INFO": "\033[0;34m",
        "SUCCESS": "\033[0;32m", 
        "WARNING": "\033[1;33m",
        "ERROR": "\033[0;31m"
    }
    color = colors.get(level, "")
    reset = "\033[0m"
    print(f"{color}[{level}]{reset} {message}")

def check_environment():
    """æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ"""
    log("æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ...")
    
    # Pythonç‰ˆæœ¬
    python_version = f"{sys.version_info.major}.{sys.version_info.minor}"
    log(f"Pythonç‰ˆæœ¬: {python_version}")
    
    # PyTorchç‰ˆæœ¬
    try:
        import torch
        torch_version = torch.__version__
        cuda_version = torch.version.cuda
        log(f"PyTorchç‰ˆæœ¬: {torch_version}")
        log(f"Torch CUDAç‰ˆæœ¬: {cuda_version}")
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            log(f"GPU: {gpu_name}")
        else:
            log("æœªæ£€æµ‹åˆ°CUDA GPU", "WARNING")
    except ImportError:
        log("PyTorchæœªå®‰è£…", "ERROR")
        return False
    
    # CUDAç¼–è¯‘å™¨
    nvcc_available, nvcc_version, _ = run_command("nvcc --version", capture_output=True, check=False)
    if nvcc_available:
        log(f"NVCC: {nvcc_version.split('release')[1].split(',')[0].strip()}")
    else:
        log("NVCCç¼–è¯‘å™¨æœªæ‰¾åˆ°", "WARNING")
    
    # ç£ç›˜ç©ºé—´
    disk_usage = shutil.disk_usage("/")
    free_gb = disk_usage.free / (1024**3)
    log(f"å¯ç”¨ç£ç›˜ç©ºé—´: {free_gb:.1f}GB")
    
    if free_gb < 5:
        log("ç£ç›˜ç©ºé—´ä¸è¶³5GBï¼Œå¯èƒ½å½±å“ç¼–è¯‘å®‰è£…", "WARNING")
    
    return True

def test_flash_attn():
    """æµ‹è¯•Flash-Attentionå®‰è£…"""
    log("æµ‹è¯•Flash-Attentionå®‰è£…...")
    
    try:
        import flash_attn
        log(f"âœ… flash-attnå¯¼å…¥æˆåŠŸï¼Œç‰ˆæœ¬: {flash_attn.__version__}", "SUCCESS")
        
        from flash_attn import flash_attn_func
        log("âœ… flash_attn_funcå¯ç”¨", "SUCCESS")
        
        from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input
        log("âœ… bert_paddingæ¨¡å—å¯ç”¨", "SUCCESS")
        
        log("ğŸ‰ Flash-AttentionåŠŸèƒ½å®Œæ•´!", "SUCCESS")
        return True
        
    except ImportError as e:
        log(f"âŒ Flash-Attentionå¯¼å…¥å¤±è´¥: {e}", "ERROR")
        return False
    except Exception as e:
        log(f"âš ï¸ Flash-Attentionéƒ¨åˆ†åŠŸèƒ½å¼‚å¸¸: {e}", "WARNING")
        return False

def cleanup_cache():
    """æ¸…ç†ç¼“å­˜"""
    log("æ¸…ç†pipç¼“å­˜...")
    
    # pipç¼“å­˜
    subprocess.run(["pip", "cache", "purge"], check=False)
    
    # ä¸´æ—¶æ–‡ä»¶
    temp_dirs = ["/tmp", "/var/tmp", os.path.expanduser("~/.cache")]
    for temp_dir in temp_dirs:
        if os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
                os.makedirs(temp_dir, exist_ok=True)
            except:
                pass
    
    log("ç¼“å­˜æ¸…ç†å®Œæˆ", "SUCCESS")

def install_precompiled():
    """å®‰è£…é¢„ç¼–è¯‘ç‰ˆæœ¬"""
    log("å¼€å§‹é¢„ç¼–è¯‘ç‰ˆæœ¬å®‰è£…...")
    
    # å®‰è£…æ–¹æ¡ˆåˆ—è¡¨ï¼ˆæŒ‰æ¨èç¨‹åº¦æ’åºï¼‰
    install_options = [
        {
            "name": "vllm-flash-attn (VLLMä¼˜åŒ–ç‰ˆ)",
            "command": "pip install --no-cache-dir vllm-flash-attn==2.6.2"
        },
        {
            "name": "å®˜æ–¹é¢„ç¼–è¯‘ç‰ˆæœ¬",
            "command": "pip install --no-cache-dir flash-attn==2.6.3 --find-links https://download.pytorch.org/whl/torch_stable.html"
        },
        {
            "name": "CUDA 11.8ä¸“ç”¨ç‰ˆæœ¬",
            "command": "pip install --no-cache-dir https://github.com/jllllll/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1%2Bcu118torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl"
        },
        {
            "name": "æœ€æ–°ç¨³å®šç‰ˆæœ¬",
            "command": "pip install --no-cache-dir flash-attn==2.7.4.post1"
        }
    ]
    
    for option in install_options:
        log(f"å°è¯•å®‰è£…: {option['name']}")
        success, _, error = run_command(option['command'], check=False)
        
        if success:
            log(f"âœ… {option['name']} å®‰è£…æˆåŠŸ", "SUCCESS")
            return True
        else:
            log(f"âŒ {option['name']} å®‰è£…å¤±è´¥: {error}", "ERROR")
    
    return False

def install_from_source():
    """ä»æºç ç¼–è¯‘å®‰è£…"""
    log("å¼€å§‹æºç ç¼–è¯‘å®‰è£…...")
    log("æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦1-3å°æ—¶ï¼Œè¯·è€å¿ƒç­‰å¾…...", "WARNING")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env.update({
        "TORCH_CUDA_ARCH_LIST": "7.0;7.5;8.0;8.6;8.9;9.0",
        "FLASH_ATTENTION_FORCE_BUILD": "TRUE",
        "MAX_JOBS": "4"
    })
    
    log("è®¾ç½®ç¼–è¯‘ç¯å¢ƒå˜é‡...")
    for key, value in env.items():
        if key.startswith(("TORCH_", "FLASH_", "MAX_")):
            log(f"{key}={value}")
    
    # ç¼–è¯‘å®‰è£…
    cmd = "pip install --no-cache-dir --no-build-isolation flash-attn==2.7.4.post1"
    process = subprocess.Popen(cmd, shell=True, env=env)
    
    try:
        return_code = process.wait()
        if return_code == 0:
            log("âœ… æºç ç¼–è¯‘å®‰è£…æˆåŠŸ", "SUCCESS")
            return True
        else:
            log("âŒ æºç ç¼–è¯‘å®‰è£…å¤±è´¥", "ERROR")
            return False
    except KeyboardInterrupt:
        log("ç”¨æˆ·ä¸­æ–­ç¼–è¯‘è¿‡ç¨‹", "WARNING")
        process.terminate()
        return False

def main():
    """ä¸»å®‰è£…æµç¨‹"""
    print("ğŸš€ Flash-Attention Pythonå®‰è£…è„šæœ¬")
    print("=" * 40)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        log("ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œé€€å‡ºå®‰è£…", "ERROR")
        return 1
    
    # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
    if test_flash_attn():
        log("Flash-Attentionå·²å®‰è£…ä¸”åŠŸèƒ½æ­£å¸¸", "SUCCESS")
        return 0
    
    # æ¸…ç†ç¼“å­˜
    cleanup_cache()
    
    # å°è¯•é¢„ç¼–è¯‘å®‰è£…
    log("å¼€å§‹å°è¯•é¢„ç¼–è¯‘å®‰è£…...")
    if install_precompiled():
        if test_flash_attn():
            log("ğŸ‰ é¢„ç¼–è¯‘å®‰è£…æˆåŠŸ!", "SUCCESS")
            return 0
        else:
            log("é¢„ç¼–è¯‘å®‰è£…å®Œæˆä½†æµ‹è¯•å¤±è´¥", "WARNING")
    
    # æ¸…ç†å¤±è´¥çš„å®‰è£…
    subprocess.run(["pip", "uninstall", "-y", "flash-attn", "vllm-flash-attn"], 
                  check=False, capture_output=True)
    cleanup_cache()
    
    # è¯¢é—®æ˜¯å¦è¿›è¡Œæºç ç¼–è¯‘
    print("\né¢„ç¼–è¯‘å®‰è£…å¤±è´¥ï¼Œæ˜¯å¦å°è¯•æºç ç¼–è¯‘ï¼Ÿ")
    print("è­¦å‘Šï¼šæºç ç¼–è¯‘å¯èƒ½éœ€è¦1-3å°æ—¶")
    choice = input("ç»§ç»­ç¼–è¯‘å®‰è£…? (y/N): ").lower().strip()
    
    if choice in ['y', 'yes']:
        log("å¼€å§‹æºç ç¼–è¯‘...")
        if install_from_source():
            if test_flash_attn():
                log("ğŸ‰ æºç ç¼–è¯‘å®‰è£…æˆåŠŸ!", "SUCCESS")
                return 0
    
    # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
    log("âŒ æ‰€æœ‰å®‰è£…æ–¹æ³•éƒ½å¤±è´¥äº†", "ERROR")
    print("\nå»ºè®®:")
    print("1. æ£€æŸ¥CUDAç¯å¢ƒæ˜¯å¦æ­£ç¡®å®‰è£…")
    print("2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å’Œå†…å­˜")  
    print("3. é¡¹ç›®å¯ä»¥åœ¨æ²¡æœ‰flash-attnçš„æƒ…å†µä¸‹è¿è¡Œï¼ˆä½¿ç”¨PyTorchåŸç”Ÿattentionï¼‰")
    
    return 1

if __name__ == "__main__":
    sys.exit(main()) 